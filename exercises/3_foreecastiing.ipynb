{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Forecasting methods (II/II)\n",
    "\n",
    "Last week, we analyzed simulated data. Today, we wlil focus on the dataset from the Kaggle competition.\n",
    "\n",
    "We will compare various neural network architectures, including multilayer perceptraons (MLP), convolutional neural networks (CNN) and recurrent neural networks (RNN).\n",
    "\n",
    "Your task is to understand the role of the different hyperparameters for each architecture. Then, you should identify the best hyperparameters to generate out-of-sample forecasts.\n",
    "\n",
    "Some important hyperparameters include: LAG (the number of lagged values), LATENT_DIM (the number of units in the layer), BATCH_SIZE (number of samples per mini-batch), EPOCHS (the number of epochs), the optimizer and the early stop strategy.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [10, 5]\n",
    "import os\n",
    "print(os.getcwd())\n",
    "import datetime as dt\n",
    "\n",
    "from main.utils.utils_methods import *\n",
    "from main.utils.utils import *\n",
    "\n",
    "import keras\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers import GRU, SimpleRNN, LSTM\n",
    "\n",
    "\n",
    "from main.module.mlp_multioutput import mlp_multioutput\n",
    "from main.module.cnn_dilated import cnn_dilated\n",
    "from main.module.rnn_vector_output import rnn_vector_output\n",
    "from main.module.rnn_encoder_decoder import rnn_encoder_decoder\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Kaggle training data\n",
    "data_all = pd.read_csv(\"../compet_data/public/train.csv\", index_col = \"Day\", parse_dates = True)\n",
    "data_all = data_all.asfreq(\"D\")\n",
    "data_all.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the time series to work with: \"series-1\", \"series-2\", ..., \"series-90\" \n",
    "which_series =  \"series-1\"\n",
    "data_raw = data_all[which_series].rename(\"traffic\").to_frame()\n",
    "data_raw.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting dates for Validation and test data\n",
    "valid_start_dt = '2017-04-01'\n",
    "test_start_dt = '2017-07-15'\n",
    "\n",
    "# Clean traininig and validation data\n",
    "learn_raw = data_raw.copy()[data_raw.index < test_start_dt][['traffic']]\n",
    "\n",
    "learn_cleaned_1, _ = clean(learn_raw.squeeze())\n",
    "learn_cleaned_2, _ = clean(learn_cleaned_1)\n",
    "learn_cleaned = learn_cleaned_2.copy().to_frame()\n",
    "learn_cleaned.plot()\n",
    "\n",
    "data_cleaned = data_raw.copy()\n",
    "data_cleaned.loc[data_cleaned.index < test_start_dt] = learn_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HORIZON = 7\n",
    "LAG = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We save and build input-output pairs for the (raw) test data \n",
    "\n",
    "look_back_dt = dt.datetime.strptime(test_start_dt, '%Y-%m-%d') - dt.timedelta(days= LAG)\n",
    "test_raw = data_raw.copy()[(data_raw.index >=look_back_dt)][['traffic']]\n",
    "\n",
    "tensor_structure = {'encoder_input':(range(-LAG+1, 1), ['traffic']), 'decoder_input':(range(0, HORIZON), ['traffic'])}\n",
    "test_inputs_raw, _, _   = X_y(test_raw, HORIZON, tensor_structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What kind of transformations do you want to apply?\n",
    "log_transform = True\n",
    "seasonal_difference = True\n",
    "\n",
    "data_before_diff = data_cleaned.copy()\n",
    "if log_transform:\n",
    "    data_before_diff = np.log(data_before_diff)\n",
    "    data_before_diff.plot()\n",
    "\n",
    "data_final = data_before_diff.copy()\n",
    "if seasonal_difference:\n",
    "    #\n",
    "    m = 7\n",
    "    assert(HORIZON % m == 0) # Simpler to implement\n",
    "    nb_season = int(HORIZON/m)\n",
    "    data_final = data_final.diff(m) # carreful pd.diff vs np.diff\n",
    "\n",
    "    #\n",
    "    reference_data = list()\n",
    "    list_dates = pd.date_range(dt.datetime.strptime(test_start_dt, '%Y-%m-%d') - dt.timedelta(days=1), data_cleaned.index[-1] - dt.timedelta(days=HORIZON))\n",
    "    for i, origin_dt in enumerate(list_dates):    \n",
    "        season = data_before_diff.loc[pd.date_range(origin_dt  - dt.timedelta(days= m - 1), origin_dt)] \n",
    "        seasonal_reference = np.tile(season[\"traffic\"].values, nb_season)\n",
    "        reference_data.append(seasonal_reference)\n",
    "    reference_season = np.vstack( reference_data)\n",
    "\n",
    "\n",
    "data_final.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING/VALIDATION/TEST DATA (TRANSFORMED)\n",
    "train = data_final.copy()[data_final.index < valid_start_dt][['traffic']]\n",
    "\n",
    "look_back_dt = dt.datetime.strptime(valid_start_dt, '%Y-%m-%d') - dt.timedelta(days=LAG)\n",
    "valid = data_final.copy()[(data_final.index >=look_back_dt)][['traffic']]\n",
    "\n",
    "look_back_dt = dt.datetime.strptime(test_start_dt, '%Y-%m-%d') - dt.timedelta(days= LAG)\n",
    "test = data_final.copy()[(data_final.index >=look_back_dt)][['traffic']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train)\n",
    "plt.plot(valid)\n",
    "plt.plot(test) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMBEDDING\n",
    "train_inputs, valid_inputs, test_inputs, X_train, y_train, X_valid, y_valid, X_test, y_test = embed_data(train, valid, test, HORIZON, LAG)\n",
    "\n",
    "train_inputs.dataframe.head()\n",
    "valid_inputs.dataframe.head()\n",
    "test_inputs.dataframe.head()"
   ]
  },
  {
   "source": [
    "Choose which loss function you want to experiment with. It is used later in the code to fit and evaluate a neural network model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function to be used to optimize the model parameters\n",
    "loss_fct = 'mse' # 'mae'\n",
    "# Accuracy measure to be used to evaluate test predictions.\n",
    "accuracy_measure = mse # mae # mape # smape\n",
    "\n",
    "# True values\n",
    "true_values = pd.DataFrame(test_inputs[\"target\"], columns=['t+'+str(t) for t in range(1, HORIZON+1)])\n"
   ]
  },
  {
   "source": [
    "## Recursive MLP\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "file_header = \"model_\" + \"mlp_recursive\"\n",
    "verbose = 0\n",
    "\n",
    "optimizer_adam = keras.optimizers.Adam(learning_rate=0.01) \n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience= 100)\n",
    "\n",
    "LATENT_DIM = 5   # number of units in the RNN layer\n",
    "BATCH_SIZE = 32  # number of samples per mini-batch\n",
    "EPOCHS = 100      # maximum number of times the training algorithm will cycle through all samples\n",
    "loss = loss_fct\n",
    "\n",
    "best_val = ModelCheckpoint('../work/' + file_header + '_{epoch:02d}.h5', save_best_only=True, mode='min', period=1)\n",
    "#########################\n",
    " \n",
    "_, _, _, X_train_onestep, y_train_onestep, X_valid_onestep, y_valid_onestep, _, _ = embed_data(train, valid, test, 1, LAG, freq = None, variable = 'traffic')\n",
    "model_mlp_recursive, history_mlp_recursive = mlp_multioutput(X_train_onestep, y_train_onestep, X_valid_onestep, y_valid_onestep, \n",
    "                        LATENT_DIM = LATENT_DIM, \n",
    "                        BATCH_SIZE = BATCH_SIZE, \n",
    "                        EPOCHS = EPOCHS, \n",
    "                        LAG = LAG, \n",
    "                        HORIZON = 1, \n",
    "                        loss = loss, \n",
    "                        optimizer = optimizer_adam,\n",
    "                        earlystop = earlystop, \n",
    "                        best_val = best_val,\n",
    "                        verbose=verbose)\n",
    "plot_learning_curves(history_mlp_recursive)\n",
    "\n",
    "best_epoch = np.argmin(np.array(history_mlp_recursive.history['val_loss']))+1\n",
    "filepath = '../work/' + file_header + '_{:02d}.h5'\n",
    "model_mlp_recursive.load_weights(filepath.format(best_epoch))\n",
    "\n",
    "#\n",
    "for h in range(HORIZON):\n",
    "    pred = model_mlp_recursive.predict(X_test)\n",
    "    X_test = pd.DataFrame(np.hstack( (np.delete(X_test.to_numpy(), 0, 1), pred) ), index = X_test.index, columns =X_test.columns)\n",
    "    if h > 0:\n",
    "        predictions = np.hstack( (predictions, pred) )\n",
    "    else:\n",
    "        predictions = pred\n",
    "\n",
    "# \n",
    "if seasonal_difference:\n",
    "    predictions = predictions + reference_season\n",
    "\n",
    "if log_transform:\n",
    "    predictions = np.exp(predictions)\n",
    "\n",
    "predictions_mlp_recursive = predictions\n",
    "\n",
    "predictions_mlp_recursive = pd.DataFrame(predictions_mlp_recursive, columns=['t+'+str(t) for t in range(1, HORIZON+1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_values = pd.DataFrame(test_inputs_raw[\"target\"], columns=['t+'+str(t) for t in range(1, HORIZON+1)])\n",
    "\n",
    "results_mlp_recursive = list()\n",
    "\n",
    "for h in range(1, HORIZON+1):\n",
    "    time_horizon = 't+'+ str(h)\n",
    "    results_mlp_recursive.append(accuracy_measure(true_values[time_horizon], predictions_mlp_recursive[time_horizon]))\n",
    "\n",
    "print(results_mlp_recursive)\n",
    "print(np.mean(results_mlp_recursive))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(true_values[\"t+1\"], \"-o\")\n",
    "plt.plot(predictions_mlp_recursive[\"t+1\"], \"-o\")"
   ]
  },
  {
   "source": [
    "# Multioutput MLP"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "file_header = \"model_\" + \"mlp_multioutput\"\n",
    "verbose = 0\n",
    "\n",
    "optimizer_adam = keras.optimizers.Adam(learning_rate=0.01) \n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience= 100)\n",
    "\n",
    "LATENT_DIM = 5   # number of units in the RNN layer\n",
    "BATCH_SIZE = 32  # number of samples per mini-batch\n",
    "EPOCHS = 100      # maximum number of times the training algorithm will cycle through all samples\n",
    "loss = loss_fct\n",
    "\n",
    "best_val = ModelCheckpoint('../work/' + file_header + '_{epoch:02d}.h5', save_best_only=True, mode='min', period=1)\n",
    "#########################\n",
    "\n",
    "model_mlp_multioutput, history_mlp_multioutput = mlp_multioutput(X_train, y_train, X_valid, y_valid, \n",
    "                        LATENT_DIM = LATENT_DIM, \n",
    "                        BATCH_SIZE = BATCH_SIZE, \n",
    "                        EPOCHS = EPOCHS, \n",
    "                        LAG = LAG, \n",
    "                        HORIZON = HORIZON, \n",
    "                        loss = loss, \n",
    "                        optimizer = optimizer_adam,\n",
    "                        earlystop = earlystop, \n",
    "                        best_val = best_val,\n",
    "                        verbose=verbose)\n",
    "plot_learning_curves(history_mlp_multioutput)\n",
    "\n",
    "best_epoch = np.argmin(np.array(history_mlp_multioutput.history['val_loss']))+1\n",
    "filepath = '../work/' + file_header + '_{:02d}.h5'\n",
    "model_mlp_multioutput.load_weights(filepath.format(best_epoch))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "predictions_mlp_multioutput = model_mlp_multioutput.predict(X_test)\n",
    "\n",
    "if seasonal_difference:\n",
    "    predictions_mlp_multioutput = predictions_mlp_multioutput + reference_season\n",
    "\n",
    "if log_transform:\n",
    "    predictions_mlp_multioutput = np.exp(predictions_mlp_multioutput)\n",
    "\n",
    "predictions_mlp_multioutput = pd.DataFrame(predictions_mlp_multioutput, columns=['t+'+str(t) for t in range(1, HORIZON+1)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_mlp_multioutput = list()\n",
    "\n",
    "for h in range(1, HORIZON+1):\n",
    "    time_horizon = 't+'+ str(h)\n",
    "    results_mlp_multioutput.append(accuracy_measure(true_values[time_horizon], predictions_mlp_multioutput[time_horizon]))\n",
    "\n",
    "print(results_mlp_multioutput)\n",
    "print(np.mean(results_mlp_multioutput))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(true_values[\"t+1\"], \"-o\")\n",
    "plt.plot(predictions_mlp_multioutput[\"t+1\"], \"-o\")"
   ]
  },
  {
   "source": [
    "# 1-D Convolutional Neural Networks (CNN)\n",
    "\n",
    "Read and try to understand the function *cnn_dilated*. You can try different number of filters and filter sizes."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "file_header = \"model_\" + \"cnn\"\n",
    "verbose = 0\n",
    "\n",
    "\n",
    "#optimizer_adam = keras.optimizers.Adam(learning_rate=0.01) \n",
    "optimizer_rmsprop = 'RMSprop'\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience= 100)\n",
    "\n",
    "LATENT_DIM = 5   # number of units in the RNN layer\n",
    "\n",
    "KERNEL_SIZE = 2 # for CNN\n",
    "\n",
    "BATCH_SIZE = 32  # number of samples per mini-batch\n",
    "EPOCHS = 100      # maximum number of times the training algorithm will cycle through all samples\n",
    "loss = loss_fct\n",
    "\n",
    "best_val = ModelCheckpoint('../work/' + file_header + '_{epoch:02d}.h5', save_best_only=True, mode='min', period=1)\n",
    "#########################\n",
    "\n",
    "model_cnn, history_cnn = cnn_dilated(train_inputs, valid_inputs, \n",
    "                        LATENT_DIM = LATENT_DIM, \n",
    "                        KERNEL_SIZE = KERNEL_SIZE,\n",
    "                        BATCH_SIZE = BATCH_SIZE, \n",
    "                        EPOCHS = EPOCHS, \n",
    "                        LAG = LAG, \n",
    "                        HORIZON = HORIZON, \n",
    "                        loss = loss, \n",
    "                        optimizer = optimizer_rmsprop,\n",
    "                        earlystop = earlystop, \n",
    "                        best_val = best_val,\n",
    "                        verbose=verbose)\n",
    "\n",
    "plot_learning_curves(history_cnn)\n",
    "\n",
    "best_epoch = np.argmin(np.array(history_cnn.history['val_loss']))+1\n",
    "filepath = '../work/' + file_header + '_{:02d}.h5'\n",
    "model_cnn.load_weights(filepath.format(best_epoch))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_cnn = model_cnn.predict(test_inputs['encoder_input'])\n",
    "\n",
    "if seasonal_difference:\n",
    "    predictions_cnn = predictions_cnn + reference_season\n",
    "\n",
    "if log_transform:\n",
    "    predictions_cnn = np.exp(predictions_cnn)\n",
    "\n",
    "predictions_cnn = pd.DataFrame(predictions_cnn, columns=['t+'+str(t) for t in range(1, HORIZON+1)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_cnn = list()\n",
    "\n",
    "for h in range(1, HORIZON+1):\n",
    "    time_horizon = 't+'+ str(h)\n",
    "    results_cnn.append(accuracy_measure(true_values[time_horizon], predictions_cnn[time_horizon]))\n",
    "\n",
    "print(results_cnn)\n",
    "print(np.mean(results_cnn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(true_values[\"t+1\"], \"-o\")\n",
    "plt.plot(predictions_cnn[\"t+1\"], \"-o\")"
   ]
  },
  {
   "source": [
    "# RNN vector-output\n",
    "\n",
    "Read and try to understand the function *rnn_vector_output*. You can try different RNN architectures (GRU, LSTM)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#########################\n",
    "file_header = \"model_\" + \"rnn_vector_output\"\n",
    "verbose = 0\n",
    "\n",
    "optimizer_adam = keras.optimizers.Adam(learning_rate=0.01) \n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience= 100)\n",
    "\n",
    "RECURRENT_MODEL = GRU #SimpleRNN # GRU # LSTM\n",
    "\n",
    "LATENT_DIM = 5   # number of units in the RNN layer\n",
    "BATCH_SIZE = 32  # number of samples per mini-batch\n",
    "EPOCHS = 100      # maximum number of times the training algorithm will cycle through all samples\n",
    "loss = loss_fct\n",
    "\n",
    "best_val = ModelCheckpoint('../work/' + file_header + '_{epoch:02d}.h5', save_best_only=True, mode='min', period=1)\n",
    "#########################\n",
    "\n",
    "model_rnn_vector_output, history_rnn_vector_output = rnn_vector_output(train_inputs, valid_inputs, \n",
    "                        RECURRENT_MODEL = RECURRENT_MODEL,\n",
    "                        LATENT_DIM = LATENT_DIM, \n",
    "                        BATCH_SIZE = BATCH_SIZE, \n",
    "                        EPOCHS = EPOCHS, \n",
    "                        LAG = LAG, \n",
    "                        HORIZON = HORIZON, \n",
    "                        loss = loss, \n",
    "                        optimizer = optimizer_adam,\n",
    "                        earlystop = earlystop, \n",
    "                        best_val = best_val,\n",
    "                        verbose=verbose)\n",
    "\n",
    "plot_learning_curves(history_rnn_vector_output)\n",
    "\n",
    "best_epoch = np.argmin(np.array(history_rnn_vector_output.history['val_loss']))+1\n",
    "filepath = '../work/' + file_header + '_{:02d}.h5'\n",
    "model_rnn_vector_output.load_weights(filepath.format(best_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_rnn_vector_output = model_rnn_vector_output.predict(test_inputs['encoder_input'])\n",
    "\n",
    "if seasonal_difference:\n",
    "    predictions_rnn_vector_output = predictions_rnn_vector_output + reference_season\n",
    "\n",
    "if log_transform:\n",
    "    predictions_rnn_vector_output = np.exp(predictions_rnn_vector_output)\n",
    "\n",
    "predictions_rnn_vector_output = pd.DataFrame(predictions_rnn_vector_output, columns=['t+'+str(t) for t in range(1, HORIZON+1)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_rnn_vector_output  = list()\n",
    "\n",
    "for h in range(1, HORIZON+1):\n",
    "    time_horizon = 't+'+ str(h)\n",
    "    results_rnn_vector_output.append(accuracy_measure(true_values[time_horizon], predictions_rnn_vector_output[time_horizon]))\n",
    "\n",
    "print(results_rnn_vector_output)\n",
    "print(np.mean(results_rnn_vector_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(true_values[\"t+1\"], \"-o\")\n",
    "plt.plot(predictions_rnn_vector_output[\"t+1\"], \"-o\")\n"
   ]
  },
  {
   "source": [
    "# RNN encoder-decoder\n",
    "\n",
    "Read and try to understand the function *rnn_encoder_decoder*. You can try different RNN architectures (GRU, LSTM)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#########################\n",
    "file_header = \"model_\" + \"rnn_encoder_decoder\"\n",
    "verbose = 0\n",
    "\n",
    "optimizer_adam = keras.optimizers.Adam(learning_rate=0.01) \n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience= 100)\n",
    "\n",
    "RECURRENT_MODEL = GRU #SimpleRNN # GRU # LSTM\n",
    "\n",
    "LATENT_DIM = 5   # number of units in the RNN layer\n",
    "BATCH_SIZE = 32  # number of samples per mini-batch\n",
    "EPOCHS = 100      # maximum number of times the training algorithm will cycle through all samples\n",
    "loss = loss_fct\n",
    "\n",
    "best_val = ModelCheckpoint('../work/' + file_header + '_{epoch:02d}.h5', save_best_only=True, mode='min', period=1)\n",
    "#########################\n",
    "\n",
    "model_rnn_encoder_decoder, history_rnn_encoder_decoder = rnn_encoder_decoder(train_inputs, valid_inputs, \n",
    "                        RECURRENT_MODEL = RECURRENT_MODEL,\n",
    "                        LATENT_DIM = LATENT_DIM, \n",
    "                        BATCH_SIZE = BATCH_SIZE, \n",
    "                        EPOCHS = EPOCHS, \n",
    "                        LAG = LAG, \n",
    "                        HORIZON = HORIZON, \n",
    "                        loss = loss, \n",
    "                        optimizer = optimizer_rmsprop,\n",
    "                        earlystop = earlystop, \n",
    "                        best_val = best_val,\n",
    "                        verbose=verbose)\n",
    "\n",
    "plot_learning_curves(history_rnn_encoder_decoder)\n",
    "\n",
    "best_epoch = np.argmin(np.array(history_rnn_encoder_decoder.history['val_loss']))+1\n",
    "filepath = '../work/' + file_header + '_{:02d}.h5'\n",
    "model_rnn_encoder_decoder.load_weights(filepath.format(best_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_rnn_encoder_decoder= model_rnn_encoder_decoder.predict(test_inputs['encoder_input'])\n",
    "\n",
    "if seasonal_difference:\n",
    "    predictions_rnn_encoder_decoder = predictions_rnn_encoder_decoder + reference_season\n",
    "\n",
    "if log_transform:\n",
    "    predictions_rnn_encoder_decoder = np.exp(predictions_rnn_encoder_decoder)\n",
    "\n",
    "predictions_rnn_encoder_decoder = pd.DataFrame(predictions_rnn_encoder_decoder, columns=['t+'+str(t) for t in range(1, HORIZON+1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_rnn_encoder_decoder  = list()\n",
    "\n",
    "for h in range(1, HORIZON+1):\n",
    "    time_horizon = 't+'+ str(h)\n",
    "    results_rnn_encoder_decoder.append(accuracy_measure(true_values[time_horizon], predictions_rnn_encoder_decoder[time_horizon]))\n",
    "\n",
    "print(results_rnn_encoder_decoder)\n",
    "print(np.mean(results_rnn_encoder_decoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(true_values[\"t+1\"], \"-o\")\n",
    "plt.plot(predictions_rnn_encoder_decoder[\"t+1\"], \"-o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forecast errors over the forecast horizon\n",
    "plt.plot(results_mlp_recursive, color='green', marker='o')\n",
    "plt.plot(results_mlp_multioutput, color='blue', marker='o')\n",
    "plt.plot(results_cnn, color='orange', marker='o')\n",
    "plt.plot(results_rnn_vector_output, color='red', marker='o')\n",
    "plt.plot(results_rnn_encoder_decoder, color='black', marker='o')\n",
    "\n"
   ]
  },
  {
   "source": [
    "We encourage you to try other combinations of transformations and forecasting strategies. Note that certain series might need diifferent transformations."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}